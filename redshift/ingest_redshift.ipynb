{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "from sagemaker import get_execution_role\n",
    "import os\n",
    "from sklearn.datasets import *\n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError\n",
    "import awswrangler as wr\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get region\n",
    "session = boto3.session.Session()\n",
    "region_name = session.region_name\n",
    "\n",
    "# Get SageMaker session & default S3 bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()  # replace with your own bucket name if you have one\n",
    "role = 'sagemaker_developer'\n",
    "prefix = \"data/tabular/california_housing\"\n",
    "filename = \"california_housing.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "sts = boto3.client(\"sts\")\n",
    "redshift = boto3.client(\"redshift\")\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "s3 = sagemaker_session.boto_session.resource(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Role name used to create this notebook is: sagemaker_developer\n"
     ]
    }
   ],
   "source": [
    "role_name = role.split(\"/\")[-1]\n",
    "print(\"Your Role name used to create this notebook is: {}\".format(role_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to upload data to s3\n",
    "def write_to_s3(filename, bucket, prefix):\n",
    "    # put one file in a separate folder. This is helpful if you read and prepare data with Athena\n",
    "    filename_key = filename.split(\".\")[0]\n",
    "    key = \"{}/{}/{}\".format(prefix, filename_key, filename)\n",
    "    return s3.Bucket(bucket).upload_file(filename, key)\n",
    "\n",
    "\n",
    "def upload_to_s3(bucket, prefix, filename):\n",
    "    url = \"s3://{}/{}/{}\".format(bucket, prefix, filename)\n",
    "    print(\"Writing to {}\".format(url))\n",
    "    write_to_s3(filename, bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to s3://sagemaker-us-east-1-083839308414/data/tabular/california_housing.csv\n"
     ]
    }
   ],
   "source": [
    "tabular_data = fetch_california_housing()\n",
    "tabular_data_full = pd.DataFrame(tabular_data.data, columns=tabular_data.feature_names)\n",
    "tabular_data_full[\"target\"] = pd.DataFrame(tabular_data.target)\n",
    "tabular_data_full.to_csv(\"california_housing.csv\", index=False)\n",
    "\n",
    "upload_to_s3(bucket, \"data/tabular\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assume_role_policy_doc = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": \"redshift.amazonaws.com\"},\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "        }\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role already exists\n"
     ]
    }
   ],
   "source": [
    "# Create Role\n",
    "iam_redshift_role_name = \"Tabular_Redshift\"\n",
    "try:\n",
    "    iam_role_redshift = iam.create_role(\n",
    "        RoleName=iam_redshift_role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(assume_role_policy_doc),\n",
    "        Description=\"Tabular data Redshift Role\",\n",
    "    )\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"Role already exists\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Role arn used to create a Redshift Cluster is: arn:aws:iam::083839308414:role/Tabular_Redshift\n"
     ]
    }
   ],
   "source": [
    "# get role arn\n",
    "role_rs = iam.get_role(RoleName=\"Tabular_Redshift\")\n",
    "iam_role_redshift_arn = role_rs[\"Role\"][\"Arn\"]\n",
    "print(\"Your Role arn used to create a Redshift Cluster is: {}\".format(iam_role_redshift_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3FullAccess\n",
    "my_redshift_to_s3 = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [{\"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": \"*\"}],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Athena Full Access\n",
    "my_redshift_to_athena = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\"Effect\": \"Allow\", \"Action\": [\"athena:*\"], \"Resource\": [\"*\"]},\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"glue:CreateDatabase\",\n",
    "                \"glue:DeleteDatabase\",\n",
    "                \"glue:GetDatabase\",\n",
    "                \"glue:GetDatabases\",\n",
    "                \"glue:UpdateDatabase\",\n",
    "                \"glue:CreateTable\",\n",
    "                \"glue:DeleteTable\",\n",
    "                \"glue:BatchDeleteTable\",\n",
    "                \"glue:UpdateTable\",\n",
    "                \"glue:GetTable\",\n",
    "                \"glue:GetTables\",\n",
    "                \"glue:BatchCreatePartition\",\n",
    "                \"glue:CreatePartition\",\n",
    "                \"glue:DeletePartition\",\n",
    "                \"glue:BatchDeletePartition\",\n",
    "                \"glue:UpdatePartition\",\n",
    "                \"glue:GetPartition\",\n",
    "                \"glue:GetPartitions\",\n",
    "                \"glue:BatchGetPartition\",\n",
    "            ],\n",
    "            \"Resource\": [\"*\"],\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetBucketLocation\",\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:ListBucketMultipartUploads\",\n",
    "                \"s3:ListMultipartUploadParts\",\n",
    "                \"s3:AbortMultipartUpload\",\n",
    "                \"s3:CreateBucket\",\n",
    "                \"s3:PutObject\",\n",
    "            ],\n",
    "            \"Resource\": [\"arn:aws:s3:::aws-athena-query-results-*\"],\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"],\n",
    "            \"Resource\": [\"arn:aws:s3:::athena-examples*\"],\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"s3:ListBucket\", \"s3:GetBucketLocation\", \"s3:ListAllMyBuckets\"],\n",
    "            \"Resource\": [\"*\"],\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"sns:ListTopics\", \"sns:GetTopicAttributes\"],\n",
    "            \"Resource\": [\"*\"],\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"cloudwatch:PutMetricAlarm\",\n",
    "                \"cloudwatch:DescribeAlarms\",\n",
    "                \"cloudwatch:DeleteAlarms\",\n",
    "            ],\n",
    "            \"Resource\": [\"*\"],\n",
    "        },\n",
    "        {\"Effect\": \"Allow\", \"Action\": [\"lakeformation:GetDataAccess\"], \"Resource\": [\"*\"]},\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    policy_redshift_s3 = iam.create_policy(\n",
    "        PolicyName=\"Tabular_RedshiftPolicyToS3\", PolicyDocument=json.dumps(my_redshift_to_s3)\n",
    "    )\n",
    "    print(\"Policy created.\")\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"Policy already exists\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)\n",
    "\n",
    "account_id = sts.get_caller_identity()[\"Account\"]\n",
    "policy_redshift_s3_arn = f\"arn:aws:iam::{account_id}:policy/Tabular_RedshiftPolicyToS3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    policy_redshift_athena = iam.create_policy(\n",
    "        PolicyName=\"Tabular_RedshiftPolicyToAthena\",\n",
    "        PolicyDocument=json.dumps(my_redshift_to_athena),\n",
    "    )\n",
    "    print(\"Policy created.\")\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"Policy already exists\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)\n",
    "\n",
    "account_id = sts.get_caller_identity()[\"Account\"]\n",
    "policy_redshift_athena_arn = f\"arn:aws:iam::{account_id}:policy/Tabular_RedshiftPolicyToAthena\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach RedshiftPolicyToAthena policy\n",
    "try:\n",
    "    response = iam.attach_role_policy(\n",
    "        PolicyArn=policy_redshift_athena_arn, RoleName=iam_redshift_role_name\n",
    "    )\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"Policy is already attached. This is ok.\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach RedshiftPolicyToS3 policy\n",
    "try:\n",
    "    response = iam.attach_role_policy(\n",
    "        PolicyArn=policy_redshift_s3_arn, RoleName=iam_redshift_role_name\n",
    "    )\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"Policy is already attached. This is ok.\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy SecretsManagerReadWrite has been succesfully attached to role: sagemaker_developer\n"
     ]
    }
   ],
   "source": [
    "# making sure you have secret manager policy attached to role\n",
    "try:\n",
    "    policy = \"SecretsManagerReadWrite\"\n",
    "    response = iam.attach_role_policy(\n",
    "        PolicyArn=\"arn:aws:iam::aws:policy/{}\".format(policy), RoleName=role_name\n",
    "    )\n",
    "    print(\"Policy %s has been succesfully attached to role: %s\" % (policy, role_name))\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"Policy is already attached.\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s \" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy AmazonRedshiftFullAccess has been succesfully attached to role: sagemaker_developer\n"
     ]
    }
   ],
   "source": [
    "# making sure you have RedshiftFullAccess policy attached to role\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "try:\n",
    "    policy = \"AmazonRedshiftFullAccess\"\n",
    "    response = iam.attach_role_policy(\n",
    "        PolicyArn=\"arn:aws:iam::aws:policy/{}\".format(policy), RoleName=role_name\n",
    "    )\n",
    "    print(\"Policy %s has been succesfully attached to role: %s\" % (policy, role_name))\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"Policy is already attached. \")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s \" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret already exists. This is ok.\n"
     ]
    }
   ],
   "source": [
    "secretsmanager = boto3.client(\"secretsmanager\")\n",
    "\n",
    "try:\n",
    "    response = secretsmanager.create_secret(\n",
    "        Name=\"tabular_redshift_login\",\n",
    "        Description=\"California Housing data New Cluster Redshift Login\",\n",
    "        SecretString='[{\"username\":\"awsuser\"},{\"password\":\"Californiahousing1\"}]',\n",
    "        Tags=[\n",
    "            {\"Key\": \"name\", \"Value\": \"tabular_redshift_login\"},\n",
    "        ],\n",
    "    )\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"ResourceExistsException\":\n",
    "        print(\"Secret already exists. This is ok.\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And retrieving the secret again\n",
    "secretsmanager = boto3.client(\"secretsmanager\")\n",
    "import json\n",
    "\n",
    "secret = secretsmanager.get_secret_value(SecretId=\"tabular_redshift_login\")\n",
    "cred = json.loads(secret[\"SecretString\"])\n",
    "\n",
    "master_user_name = cred[0][\"username\"]\n",
    "master_user_pw = cred[1][\"password\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "# Redshift configuration parameters\n",
    "redshift_cluster_identifier = \"redshiftdemo\"\n",
    "database_name = \"california_housing\"\n",
    "cluster_type = \"multi-node\"\n",
    "node_type = \"dc2.large\"\n",
    "number_nodes = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_redshift_security_group():\n",
    "    ec2 = boto3.client('ec2', region_name='us-east-1',aws_access_key_id='AKIARHBJWKJ7PKABODEC'\n",
    "                                   ,aws_secret_access_key='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')\n",
    "\n",
    "    # Each region has a unique VPC.\n",
    "    response = ec2.describe_vpcs()\n",
    "    vpc_id = response.get('Vpcs', [{}])[0].get('VpcId', '')\n",
    "    if not vpc_id:\n",
    "        raise RuntimeError(\"You must create a VPC first!\")\n",
    "\n",
    "    port = int(5439)\n",
    "    group_name = 'redshift-security-group'\n",
    "    try:\n",
    "        response = ec2.create_security_group(\n",
    "            GroupName=group_name,\n",
    "            Description='redshift security group',\n",
    "            VpcId=vpc_id)\n",
    "        security_group_id = response['GroupId']\n",
    "        print(f\"Security Group {security_group_id} Created in vpc {vpc_id}.\")\n",
    "\n",
    "        data = ec2.authorize_security_group_ingress(\n",
    "            GroupId=security_group_id,\n",
    "            IpPermissions=[\n",
    "                {'IpProtocol': 'tcp',\n",
    "                 'FromPort': port,\n",
    "                 'ToPort': port,\n",
    "                 'IpRanges': [{'CidrIp': '0.0.0.0/0'}]}\n",
    "            ])\n",
    "        print(f\"Ingress Successfully Set {data}\")\n",
    "        return security_group_id\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'InvalidGroup.Duplicate':\n",
    "            response = ec2.describe_security_groups(\n",
    "                Filters=[\n",
    "                    dict(Name='group-name', Values=[group_name])\n",
    "                ]\n",
    "            )\n",
    "            return response['SecurityGroups'][0]['GroupId']\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "security_group_id=create_redshift_security_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cluster': {'ClusterIdentifier': 'redshiftdemo', 'NodeType': 'dc2.large', 'ClusterStatus': 'creating', 'ClusterAvailabilityStatus': 'Modifying', 'MasterUsername': 'awsuser', 'DBName': 'california_housing', 'AutomatedSnapshotRetentionPeriod': 1, 'ManualSnapshotRetentionPeriod': -1, 'ClusterSecurityGroups': [], 'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-06acad5291c25b090', 'Status': 'active'}], 'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0', 'ParameterApplyStatus': 'in-sync'}], 'ClusterSubnetGroupName': 'default', 'VpcId': 'vpc-0f7f95fdb82475396', 'PreferredMaintenanceWindow': 'fri:04:30-fri:05:00', 'PendingModifiedValues': {'MasterUserPassword': '****'}, 'ClusterVersion': '1.0', 'AllowVersionUpgrade': True, 'NumberOfNodes': 2, 'PubliclyAccessible': False, 'Encrypted': False, 'Tags': [], 'EnhancedVpcRouting': False, 'IamRoles': [{'IamRoleArn': 'arn:aws:iam::083839308414:role/Tabular_Redshift', 'ApplyStatus': 'adding'}], 'MaintenanceTrackName': 'current', 'DeferredMaintenanceWindows': [], 'NextMaintenanceWindowStartTime': datetime.datetime(2023, 2, 3, 4, 30, tzinfo=tzutc()), 'AquaConfiguration': {'AquaStatus': 'disabled', 'AquaConfigurationStatus': 'auto'}}, 'ResponseMetadata': {'RequestId': '0d2ffa8a-fde7-42d6-b1f4-e05b3a78bce3', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '0d2ffa8a-fde7-42d6-b1f4-e05b3a78bce3', 'content-type': 'text/xml', 'content-length': '2472', 'date': 'Wed, 01 Feb 2023 05:58:16 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = redshift.create_cluster(\n",
    "    DBName=database_name,\n",
    "    ClusterIdentifier=redshift_cluster_identifier,\n",
    "    ClusterType=cluster_type,\n",
    "    NodeType=node_type,\n",
    "    NumberOfNodes=int(number_nodes),\n",
    "    MasterUsername=master_user_name,\n",
    "    MasterUserPassword=master_user_pw,\n",
    "    # ClusterSubnetGroupName=\"<cluster-subnet-group-1>\",  # you can either specify an existing subnet group (change this to your Subnet Group name), or use the security group ID that was retrieved above\n",
    "    IamRoles=[iam_role_redshift_arn],\n",
    "    VpcSecurityGroupIds=[security_group_id],\n",
    "    Port=5439,\n",
    "    PubliclyAccessible=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Redshift Cluster Status is: available\n"
     ]
    }
   ],
   "source": [
    "# check cluster status\n",
    "response = redshift.describe_clusters(ClusterIdentifier=redshift_cluster_identifier)\n",
    "cluster_status = response[\"Clusters\"][0][\"ClusterStatus\"]\n",
    "print(\"Your Redshift Cluster Status is: \" + cluster_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redshift endpoint: redshiftdemo.cl3ngyue18f8.us-east-1.redshift.amazonaws.com\n",
      "IAM Role: arn:aws:iam::083839308414:role/Tabular_Redshift\n"
     ]
    }
   ],
   "source": [
    "redshift_endpoint_address = response[\"Clusters\"][0][\"Endpoint\"][\"Address\"]\n",
    "iam_role = response[\"Clusters\"][0][\"IamRoles\"][0][\"IamRoleArn\"]\n",
    "\n",
    "print(\"Redshift endpoint: {}\".format(redshift_endpoint_address))\n",
    "print(\"IAM Role: {}\".format(iam_role))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redshift_cluster_endpoint():\n",
    "\n",
    "    redshift_client = boto3.client('redshift', region_name='us-east-1',aws_access_key_id='AKIARHBJWKJ7PKABODEC'\n",
    "                                   ,aws_secret_access_key='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')\n",
    "    endpoint = redshift_client.describe_clusters(\n",
    "        ClusterIdentifier='redshiftdemo')[\n",
    "        'Clusters'][0]['Endpoint']\n",
    "    return endpoint['Address'], endpoint['Port']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redshiftdemo.cl3ngyue18f8.us-east-1.redshift.amazonaws.com\n",
      "5439\n"
     ]
    }
   ],
   "source": [
    "address, port =get_redshift_cluster_endpoint()\n",
    "print(f\"Connecting to Redshift cluster at {address}:{port} ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "secretsmanager = boto3.client(\"secretsmanager\")\n",
    "secret = secretsmanager.get_secret_value(SecretId=\"tabular_redshift_login\")\n",
    "cred = json.loads(secret[\"SecretString\"])\n",
    "\n",
    "master_user_name = cred[0][\"username\"]\n",
    "master_user_pw = cred[1][\"password\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift_cluster_identifier = \"redshiftdemo\"\n",
    "\n",
    "database_name_redshift = \"california_housing\"\n",
    "database_name_athena = \"tabular_california_housing\"\n",
    "\n",
    "redshift_port = int(5439)\n",
    "\n",
    "schema_redshift = \"redshift\"\n",
    "schema_spectrum = \"spectrum\"\n",
    "\n",
    "table_name_csv = \"california_housing_athena\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_db():\n",
    "    \"\"\"Connects to the database.\"\"\"\n",
    "    address, port = get_redshift_cluster_endpoint()\n",
    "    # connect to default database\n",
    "    print(f\"Connecting to Redshift cluster at {address}:{port} ...\")\n",
    "    conn = psycopg2.connect(\n",
    "        f\"host={address} \"\n",
    "        f\"dbname={database_name_redshift} \"\n",
    "        f\"user={master_user_name} \"\n",
    "        f\"password={master_user_pw} \"\n",
    "        f\"port={port}\")\n",
    "    print(conn)\n",
    "    conn.set_session(autocommit=True)\n",
    "    return conn.cursor(), conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Redshift cluster at redshiftdemo.cl3ngyue18f8.us-east-1.redshift.amazonaws.com:5439 ...\n",
      "<connection object at 0x7f4d8c403c28; dsn: 'user=awsuser password=xxx dbname=california_housing host=redshiftdemo.cl3ngyue18f8.us-east-1.redshift.amazonaws.com port=5439', closed: 0>\n"
     ]
    }
   ],
   "source": [
    "cur, conn = connect_db()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Redshift Database Engine\n",
    "engine = create_engine(\n",
    "    \"postgresql://{}:{}@{}:{}/{}\".format(\n",
    "        master_user_name,\n",
    "        master_user_pw,\n",
    "        address,\n",
    "        port,\n",
    "        database_name_redshift,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::083839308414:role/Tabular_Redshift\n"
     ]
    }
   ],
   "source": [
    "# config session\n",
    "session = sessionmaker()\n",
    "session.configure(bind=engine)\n",
    "s = session()\n",
    "print(iam_role)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Access Data without Moving it to Redshift: Amazon Redshift Spectrum\n",
    "Redshift Spectrum is used to query data directly from files on Amazon S3.You will need to create external tables in an external schema. The external schema references a database in the external data catalog and provides the IAM role ARN that authorizes your cluster to access Amazon S3 on your behalf.\n",
    "\n",
    "Get table and schema information from the Glue Catalog: getting meta data from data catalog and connecting to the Athena database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "rollback;\n",
    "create external schema if not exists {} from data catalog \n",
    "    database '{}' \n",
    "    iam_role '{}'\n",
    "    create external database if not exists\n",
    "\"\"\".format(\n",
    "    schema_spectrum, database_name_athena, iam_role\n",
    ")\n",
    "\n",
    "s.execute(statement)\n",
    "s.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medinc</th>\n",
       "      <th>houseage</th>\n",
       "      <th>averooms</th>\n",
       "      <th>avebedrms</th>\n",
       "      <th>population</th>\n",
       "      <th>aveoccup</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>medvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   medinc  houseage  averooms  avebedrms  population  aveoccup  latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   longitude  medvalue  \n",
       "0    -122.23     4.526  \n",
       "1    -122.22     3.585  \n",
       "2    -122.24     3.521  \n",
       "3    -122.25     3.413  \n",
       "4    -122.25     3.422  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement = \"\"\"\n",
    "select *\n",
    "    from {}.{} limit 10\n",
    "\"\"\".format(\n",
    "    schema_spectrum, table_name_csv\n",
    ")\n",
    "\n",
    "df = pd.read_sql_query(statement, engine)\n",
    "df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: Loading Data into Redshift from Athena\n",
    "To load data into Redshift, you need to either use COPY command or INSERT INTO command to move data into a table from data files. Copied files may reside in an S3 bucket, an EMR cluster, or on a remote host accessed.\n",
    "\n",
    "Create and Upload Data into Athena Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_s3_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-e3afaf187305>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\\\n' LOCATION '{}'\n\u001b[1;32m     17\u001b[0m TBLPROPERTIES ('skip.header.line.count'='1')\"\"\".format(\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdatabase_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_name_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_s3_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_s3_path' is not defined"
     ]
    }
   ],
   "source": [
    "database_name = \"tabular_california_housing\"\n",
    "table_name_csv = \"california_housing_athena\"\n",
    "\n",
    "# SQL statement to execute\n",
    "statement = \"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS {}.{}(\n",
    "        MedInc double,\n",
    "        HouseAge double,\n",
    "        AveRooms double,\n",
    "        AveBedrms double,\n",
    "        Population double,\n",
    "        AveOccup double,\n",
    "        Latitude double,\n",
    "        Longitude double, \n",
    "        MedValue double\n",
    "\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\\\n' LOCATION '{}'\n",
    "TBLPROPERTIES ('skip.header.line.count'='1')\"\"\".format(\n",
    "    database_name, table_name_csv, data_s3_path\n",
    ")\n",
    "\n",
    "# Execute statement using connection cursor\n",
    "cursor = connect(region_name=region_name, s3_staging_dir=s3_staging_dir).cursor()\n",
    "cursor.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the table has been created\n",
    "statement = \"SHOW TABLES in {}\".format(database_name)\n",
    "cursor.execute(statement)\n",
    "\n",
    "df_show = as_pandas(cursor)\n",
    "df_show.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create schema\n",
    "statement = \"\"\"create schema if not exists {}\"\"\".format(schema_redshift)\n",
    "\n",
    "s = session()\n",
    "s.execute(statement)\n",
    "s.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name_redshift = table_name_csv + \"_\" + \"redshift_insert\"\n",
    "statement = \"\"\"\n",
    "rollback;\n",
    "create table if not exists redshift.{}(\n",
    "        MedInc float,\n",
    "        HouseAge float,\n",
    "        AveRooms float,\n",
    "        AveBedrms float,\n",
    "        Population float,\n",
    "        AveOccup float,\n",
    "        Latitude float,\n",
    "        Longitude float, \n",
    "        MedValue float)\"\"\".format(\n",
    "    table_name_redshift\n",
    ")\n",
    "\n",
    "s.execute(statement)\n",
    "s.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name_redshift = table_name_csv + \"_\" + \"redshift_insert\"\n",
    "\n",
    "statement = \"\"\"\n",
    "    insert into redshift.{}\n",
    "        select * from {}.{}             \n",
    "    \"\"\".format(\n",
    "    table_name_redshift, schema_spectrum, table_name_csv\n",
    ")\n",
    "s.execute(statement)\n",
    "s.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "     select * from redshift.{} limit 10\n",
    "\"\"\".format(\n",
    "    table_name_redshift\n",
    ")\n",
    "df = pd.read_sql_query(statement, engine)\n",
    "df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 3: Copy data directly from S3\n",
    "You can also Copy Data into a new table. https://docs.aws.amazon.com/redshift/latest/dg/tutorial-loading-run-copy.html\n",
    "\n",
    "Create a new Schema in Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new sample table\n",
    "table_name_redshift = table_name_csv + \"_\" + \"redshift_copy\"\n",
    "statement = \"\"\"\n",
    "rollback;\n",
    "create table if not exists redshift.{}(\n",
    "        MedInc float,\n",
    "        HouseAge float,\n",
    "        AveRooms float,\n",
    "        AveBedrms float,\n",
    "        Population float,\n",
    "        AveOccup float,\n",
    "        Latitude float,\n",
    "        Longitude float, \n",
    "        MedValue float)\"\"\".format(\n",
    "    table_name_redshift\n",
    ")\n",
    "\n",
    "s.execute(statement)\n",
    "s.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name_redshift = table_name_csv + \"_\" + \"redshift_copy\"\n",
    "data_s3_path = \"s3://{}/data/tabular/california_housing/california_housing.csv\".format(bucket)\n",
    "statement = \"\"\"\n",
    "rollback;\n",
    "copy redshift.{}  \n",
    "  from '{}'\n",
    "  iam_role '{}'\n",
    "  csv\n",
    "  ignoreheader 1\n",
    "    \"\"\".format(\n",
    "    table_name_redshift, data_s3_path, iam_role\n",
    ")\n",
    "s.execute(statement)\n",
    "s.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medinc</th>\n",
       "      <th>houseage</th>\n",
       "      <th>averooms</th>\n",
       "      <th>avebedrms</th>\n",
       "      <th>population</th>\n",
       "      <th>aveoccup</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>medvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   medinc  houseage  averooms  avebedrms  population  aveoccup  latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   longitude  medvalue  \n",
       "0    -122.23     4.526  \n",
       "1    -122.22     3.585  \n",
       "2    -122.24     3.521  \n",
       "3    -122.25     3.413  \n",
       "4    -122.25     3.422  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement = \"\"\"\n",
    "     select * from redshift.{} limit 10\n",
    "\"\"\".format(\n",
    "    table_name_redshift\n",
    ")\n",
    "df_copy = pd.read_sql_query(statement, engine)\n",
    "df_copy.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS Data Wrangler Get Engine Function\n",
    "Run this command within a private subnet. You can find your host address by going to the Redshift Console, then choose Clusters -> Property -> Connection details -> View all connection details -> Node IP address -> Private IP address. https://aws-data-wrangler.readthedocs.io/en/latest/stubs/awswrangler.db.get_engine.html#awswrangler.db.get_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Private IP address is:  172.31.65.127\n"
     ]
    }
   ],
   "source": [
    "private_ip = redshift.describe_clusters(ClusterIdentifier=redshift_cluster_identifier)[\"Clusters\"][\n",
    "    0\n",
    "][\"ClusterNodes\"][0][\"PrivateIPAddress\"]\n",
    "print(\"Private IP address is: \", private_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "connect() got an unexpected keyword argument 'db_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-8678d79e1c39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdatabase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatabase_name_redshift\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaster_user_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaster_user_pw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: connect() got an unexpected keyword argument 'db_type'"
     ]
    }
   ],
   "source": [
    "engine = wr.db.get_engine(\n",
    "    db_type=\"postgresql\",\n",
    "    host=private_ip,  # Private IP address of your Redshift Cluster\n",
    "    port=redshift_port,\n",
    "    database=database_name_redshift,\n",
    "    user=master_user_name,\n",
    "    password=master_user_pw,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wr.db.read_sql_query(\"SELECT * FROM redshift.{}\".format(table_name_redshift), con=engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0 | packaged by conda-forge | (default, Nov 12 2018, 20:15:55) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f16291dcfc90b6fff70b6932b4387d3b8faeb5f0ae59960ab932401343f437b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

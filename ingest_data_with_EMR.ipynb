{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sklearn.datasets import *\n",
    "import pandas as pd\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "s3 = sagemaker_session.boto_session.resource(\"s3\")\n",
    "bucket = sagemaker_session.default_bucket()  # replace with your own bucket name if you have one\n",
    "prefix = \"data/tabular/boston_house\"\n",
    "filename = \"boston_house.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data from online resources and write data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_s3(filename, bucket, prefix):\n",
    "    # put one file in a separate folder. This is helpful if you read and prepare data with Athena\n",
    "    filename_key = filename.split(\".\")[0]\n",
    "    key = \"{}/{}/{}\".format(prefix, filename_key, filename)\n",
    "    return s3.Bucket(bucket).upload_file(filename, key)\n",
    "\n",
    "\n",
    "def upload_to_s3(bucket, prefix, filename):\n",
    "    url = \"s3://{}/{}/{}\".format(bucket, prefix, filename)\n",
    "    print(\"Writing to {}\".format(url))\n",
    "    write_to_s3(filename, bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to s3://sagemaker-us-east-1-083839308414/data/tabular/boston_house.csv\n"
     ]
    }
   ],
   "source": [
    "tabular_data = load_boston()\n",
    "tabular_data_full = pd.DataFrame(tabular_data.data, columns=tabular_data.feature_names)\n",
    "tabular_data_full[\"target\"] = pd.DataFrame(tabular_data.target)\n",
    "tabular_data_full.to_csv(\"boston_house.csv\", index=False)\n",
    "\n",
    "upload_to_s3(bucket, \"data/tabular\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is path to your s3 files: s3://sagemaker-us-east-1-083839308414/data/tabular/boston_house/boston_house.csv\n"
     ]
    }
   ],
   "source": [
    "data_s3_path = \"s3://{}/{}/{}\".format(bucket, prefix, filename)\n",
    "print(\"this is path to your s3 files: \" + data_s3_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the S3 bucket file path\n",
    "The S3 bucket file path is required to read the data on EMR Spark. Copy and paste the path string shown above into the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace this path string with your path shown in last step\n",
    "data_s3_path = \"s3://sagemaker-us-east-2-060356833389/data/tabular/boston_house/boston_house.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data in EMR spark Cluster\n",
    "Once we have a path to our data in S3, we can use spark s3 select to read data with the following command. You can specify a data format, schema is not necessary but recommended, and in options you can specify compression, delimiter, header, etc. For more details, please see documentation on using S3 select with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5a5a75eaaeb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mCHAS\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNOX\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRM\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mAGE\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDIS\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mRAD\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mTAX\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPTRATIO\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mB\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mLSTAT\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_s3_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# EMR cell\n",
    "schema = \" CRIM double, ZN double, INDUS double,\\\n",
    "CHAS double, NOX double, RM double,  AGE double, DIS double,  RAD double,  TAX double, PTRATIO double, \\\n",
    "B double,  LSTAT double, target double\"\n",
    "df = spark.read.format(\"csv\").schema(schema).options(header=\"true\").load(data_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "Now that you have read in the data, you can pre-process the data with Spark in an EMR cluster, build an ML pipeline, and train models in scale.\n",
    "\n",
    "Citation\n",
    "Boston Housing data, Harrison, D. and Rubinfeld, D.L. `Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f16291dcfc90b6fff70b6932b4387d3b8faeb5f0ae59960ab932401343f437b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.75.1\n"
     ]
    }
   ],
   "source": [
    "# import sagemaker SDK\n",
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.14.0\n",
      "  latest version: 22.11.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/zqc/anaconda3/envs/airflow_env\n",
      "\n",
      "  added / updated specs:\n",
      "    - openjdk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    openjdk-8.0.332            |       h166bdaf_0        97.8 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        97.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  openjdk            conda-forge/linux-64::openjdk-8.0.332-h166bdaf_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "openjdk-8.0.332      | 97.8 MB   | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Retrieving notices: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# setup - install JDK\n",
    "# you only need to run this once per KernelApp\n",
    "%conda install openjdk -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n",
      "Ivy Default Cache set to: /home/zqc/.ivy2/cache\n",
      "The jars for the packages stored in: /home/zqc/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/zqc/anaconda3/envs/airflow_env/lib/python3.7/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bc8e6f2d-3529-47d3-838b-b7ade2fcc8f3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.563/aws-java-sdk-bundle-1.11.563.jar ...\n"
     ]
    }
   ],
   "source": [
    "# import PySpark and build Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySparkApp\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "    .config(\n",
    "        \"fs.s3a.aws.credentials.provider\",\n",
    "        \"com.amazonaws.auth.ContainerCredentialsProvider\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have installed PySpark and initiated a Spark session, let's try out a couple of sample Pandas user defined functions (UDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/02 17:12:48 WARN Utils: Your hostname, zhangqinchuan resolves to a loopback address: 127.0.1.1; using 172.19.186.130 instead (on interface eth0)\n",
      "23/02/02 17:12:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/02/02 17:12:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                   v|\n",
      "+---+--------------------+\n",
      "|  0| 0.34857940639844887|\n",
      "|  0|  0.2088843626214535|\n",
      "|  0|  0.9577188505061983|\n",
      "|  0|  0.7372880435600724|\n",
      "|  0|  0.2544530997135277|\n",
      "|  0| 0.46724977436394455|\n",
      "|  0|  0.5003725231636404|\n",
      "|  0|  0.6921533114907326|\n",
      "|  0| 0.38695771043189997|\n",
      "|  0|   0.508820584322134|\n",
      "|  0|  0.5095775991714272|\n",
      "|  0| 0.31687758980157477|\n",
      "|  0|  0.8122500198314806|\n",
      "|  0|   0.688595791494441|\n",
      "|  0|0.008732864399759732|\n",
      "|  0| 0.43890907196679696|\n",
      "|  0| 0.23843125300844337|\n",
      "|  0| 0.23762814800889476|\n",
      "|  0|  0.6335448406490326|\n",
      "|  0|  0.5099400202934323|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    count,\n",
    "    rand,\n",
    "    collect_list,\n",
    "    explode,\n",
    "    struct,\n",
    "    count,\n",
    "    lit,\n",
    ")\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# generate random data\n",
    "df = (\n",
    "    spark.range(0, 10 * 100 * 100)\n",
    "    .withColumn(\"id\", (col(\"id\") / 100).cast(\"integer\"))\n",
    "    .withColumn(\"v\", rand())\n",
    ")\n",
    "df.cache()\n",
    "df.count()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, v: double, v2: double]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample pandas udf to return squared value\n",
    "@pandas_udf(\"double\", PandasUDFType.SCALAR)\n",
    "def pandas_squared(v):\n",
    "    return v * v\n",
    "\n",
    "\n",
    "df.withColumn(\"v2\", pandas_squared(df.v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+-------------------+\n",
      "| id|                  y|                 x1|                 x2|\n",
      "+---+-------------------+-------------------+-------------------+\n",
      "|  0| 0.9496667131158872|0.31657918118705486| 0.7840432647540505|\n",
      "|  0|0.29494229648149173| 0.5885629332832106| 0.9091660853188566|\n",
      "|  0| 0.5482080607263939| 0.1874023705387422| 0.5685358929673487|\n",
      "|  0| 0.6007644849083354| 0.0394802673279353| 0.8216625579310718|\n",
      "|  0|0.07871132374499978| 0.6590352280143387| 0.2976351871769567|\n",
      "|  0|  0.545962237778807| 0.0756127303680253| 0.8041423922458693|\n",
      "|  0| 0.0235432683535296| 0.1644863362855733|0.42421380939402764|\n",
      "|  0| 0.5560372426946423|0.03445009987797776|0.21012616937087525|\n",
      "|  0|0.26721589373035226|  0.959004579661844| 0.2840591670386182|\n",
      "|  0| 0.2893542126215911| 0.1243963490622556|0.19768101374138214|\n",
      "|  0| 0.4059060167459927| 0.9138279344860069| 0.9681735033828731|\n",
      "|  0|0.05881735786624165|  0.629839262846955|  0.717064995534248|\n",
      "|  0| 0.5666702961075839| 0.6885416595479508|0.24975951823804332|\n",
      "|  0| 0.6099974754016346|0.29003187464351265| 0.6070015819064198|\n",
      "|  0| 0.5168720913442747| 0.4340382763497138|0.32974128730883456|\n",
      "|  0| 0.5056915301869763| 0.7124923439799071|0.26365585662886815|\n",
      "|  0| 0.2838757894970878| 0.6663622872082278| 0.8807151260620433|\n",
      "|  0| 0.2347307912602674|0.30548006684460194| 0.2905044843196455|\n",
      "|  0| 0.9009414995342676| 0.6500480128356227| 0.1979895543034519|\n",
      "|  0| 0.6492096210743131| 0.7658818857481361| 0.8049183962407905|\n",
      "+---+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = (\n",
    "    df.withColumn(\"y\", rand())\n",
    "    .withColumn(\"x1\", rand())\n",
    "    .withColumn(\"x2\", rand())\n",
    "    .select(\"id\", \"y\", \"x1\", \"x2\")\n",
    ")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels.api'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14174/3567923896.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgroup_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"id\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels.api'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "group_column = \"id\"\n",
    "y_column = \"y\"\n",
    "x_columns = [\"x1\", \"x2\"]\n",
    "schema = df2.select(group_column, *x_columns).schema"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f16291dcfc90b6fff70b6932b4387d3b8faeb5f0ae59960ab932401343f437b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
